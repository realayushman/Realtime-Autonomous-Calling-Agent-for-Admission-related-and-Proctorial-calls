import os
import streamlit as st
from dotenv import load_dotenv
from langchain.chains import RetrievalQA
from utils import get_vectorstore, set_custom_prompt, load_llm, CUSTOM_PROMPT_TEMPLATE
load_dotenv()

DB_FAISS_PATH = "vectorstore/db_faiss"
GROQ_MODEL = "openai/gpt-oss-20b"  
GROQ_API_KEY = os.environ.get("GROQ_API_KEY")

def main():
    st.title("Ask Chatbot!")

    if 'messages' not in st.session_state:
        st.session_state.messages = []

    for message in st.session_state.messages:
        st.chat_message(message['role']).markdown(message['content'])

    prompt = st.chat_input("Pass your prompt here")

    if prompt:
        st.chat_message('user').markdown(prompt)
        st.session_state.messages.append({'role': 'user', 'content': prompt})

        try:
            vectorstore = get_vectorstore()
            if vectorstore is None:
                st.error("Failed to load the vector store")

            qa_chain = RetrievalQA.from_chain_type(
                llm=load_llm(GROQ_MODEL),
                chain_type="stuff",
                retriever=vectorstore.as_retriever(search_kwargs={'k': 3}),
                return_source_documents=True,
                chain_type_kwargs={'prompt': set_custom_prompt(CUSTOM_PROMPT_TEMPLATE)}
            )

            response = qa_chain.invoke({'query': prompt})

            result = response["result"]
            #source_documents = response["source_documents"]
            result_to_show = result #+ "\nSource Docs:\n" + str(source_documents)
            st.chat_message('assistant').markdown(result_to_show)
            st.session_state.messages.append({'role': 'assistant', 'content': result_to_show})

        except Exception as e:
            st.error(f"Error: {str(e)}")

if __name__ == "__main__":
    main()